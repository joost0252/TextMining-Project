---
output:
  html_document: default
  pdf_document: default
---
```{r, echo = FALSE, message = FALSE, warning = FALSE}
source("../scripts/setup.R")
```


```{r echo=FALSE}
data_scripts <- read.csv("../data/series_scripts.csv")
```


# Tokenization and cleaning of the data 
We define the corpus for our analysis. The corpus will be the scrips column in our data set. Then we clean the texts by removing the numbers, the punctuation, the symbols, and the separators. 

In our scripts, we can see the main characters that are _Sheldon_, _Leonard_, _Penny_, _Howard_ and _Raj_ and they appear in all the seasons' episodes. Therefore, we remove their name for the following explanatory data analysis because it will otherwise bias our results. 

We noticed that the term _'Amy'_ and _'Bernadette'_ appears the most. These are characters that appear towards the end of the series. The term 'scene' is also normal that it appears a lot, because at the beginning of each script and each time the characters change the scene, the term 'scene' is written.
Therefore, we also remove these terms for the following analysis. 

```{r echo=FALSE}
script.cp <- corpus(data_scripts$script)

script.tk <- tokens(
  script.cp,
  remove_numbers = TRUE, 
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE
  )

#Remove possesive apostrophe ('s), such that words as leonard's are not skipped in the next part of the code
script.tk <- tokens_replace(
    script.tk, 
    types(script.tk), 
    stringi::stri_replace_all_regex(types(script.tk), "['\\p{Pf}][s]", "")
)

script.tk <- script.tk %>% 
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(
    c("sheldon","leonard","penny","howard",
      "raj", "amy", "bernadette", "scene", "series", "episode"))  #remove main characters'name and 'scene'
script.tk
```


# The Document-Term Matrix, the TF-IDF and the global frequencies 

```{r echo=FALSE}
## Compute the DTM
script.dfm <- dfm(script.tk)

## Compute the TF-IDF 
script.tfidf <- dfm_tfidf(script.dfm)  

## Compute the global frequencies
script.freq <- textstat_frequency(script.dfm)

## We take the top 40 terms that appear the most
script.freq %>% 
  top_n(40, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term") + 
  ggtitle("Top 40 most frequent words")
```


## Cloud of words 
The bigger the word, the more frequently it appears.

```{r echo=FALSE}
textplot_wordcloud(script.dfm)
```

## The 10 most frequent words per episode
This plot doesn't tells us much information.

```{r echo=FALSE}
script.dfm %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2) + 
  ggtitle("Top 10 TF per episodes")
```

## The TF-IDF per episode 

```{r}
script.tfidf %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2) + 
    ggtitle("Top 10 TF-IDF per episodes")
```


# Exploratory Data analysis 
We see that "yeah", "apartment", "hey" "knock" and "gonna" are the most frequent terms as we could also see before. 
Terms such as "meemaw", "lesley", "missy", "arthur" and "bert" are rather more likely to be specific to a dew texts while still somehow frequent. 

```{r echo=FALSE}
script.freq %>% 
  ggplot(aes(x=log10(docfreq),
             y=log10(frequency))) + 
  geom_text(aes(label=feature),
            position=position_jitter(),
            size=3) + 
  xlab("Episode log-frequency") + 
  ylab("log-frequency") + 
  ggtitle("Representation of the frequencies agains the episode frequencies")
```

## Lexical diversity 
We compute the lexical diversity of the scripts. 

```{r echo=FALSE}
script.dfm %>% textstat_lexdiv()
```

```{r echo=FALSE}
script.dfm %>% textstat_lexdiv() %>%
  arrange(desc(TTR)) %>%
  slice(1:10) %>% #plot only the top 10
  ggplot(aes(reorder(document, -TTR),
             TTR)) + 
  geom_bar(stat = "identity") + 
  xlab("Episodes") + 
  ggtitle("Plot of the 10 top Token-Type-Ratio") 
```

## Keyness analysis 


```{r echo=FALSE}
## text131 
script.keyness <- textstat_keyness(script.dfm, target = "text131")

textplot_keyness(script.keyness)

## text92
script.keyness <- textstat_keyness(script.dfm, target = "text92")

textplot_keyness(script.keyness)
```


```{r echo=FALSE}
#Table for co-occurance for all features
script.co <- fcm(script.tk, 
                context = "document", 
                tri = FALSE)
script.co
```


```{r}
#Table for co-occurance for features with more than 300 co-occurances
index <- script.freq %>% 
  filter(frequency > 300) %>% 
  data.frame() %>% 
  select(feature)
script.co <- script.co[index$feature, index$feature]
script.co
```


```{r echo=FALSE}
#Co-occurence graph with every line being 400 occurances. I believe we can do some more data cleaning, as these words take
#a lot of occurances, while it problably wont help for our sentiment analysis
script.co[script.co <= 400] <- 0
script.co[script.co > 400] <- 1
network <- graph_from_adjacency_matrix(
  script.co,
  mode = "undirected",
  diag = FALSE)
plot(network,
     layout = layout_with_kk)
```



################################ Season Analysis ##################################################

```{r echo=FALSE}
season.cp <- corpus(season_scripts$agg_script)

season.tk <- tokens(
  season.cp,
  remove_numbers = TRUE, 
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE
  )

#Remove possesive apostrophe ('s), such that words as leonard's are not skipped in the next part of the code
season.tk <- tokens_replace(
    season.tk, 
    types(season.tk), 
    stringi::stri_replace_all_regex(types(season.tk), "['\\p{Pf}][s]", "")
)

season.tk <- season.tk %>% 
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(
    c("sheldon","leonard","penny","howard",
      "raj", "amy", "bernadette", "scene", "series", "episode"))  #remove main characters'name and 'scene'
season.tk
```

# The Document-Term Matrix, the TF-IDF and the global frequencies 

```{r echo=FALSE}
## Compute the DTM
season.dfm <- dfm(season.tk)

## Compute the TF-IDF 
season.tfidf <- dfm_tfidf(season.dfm)  

## Compute the global frequencies
season.freq <- textstat_frequency(season.dfm)

## We take the top 40 terms that appear the most
season.freq %>% 
  top_n(40, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term") + 
  ggtitle("Top 40 most frequent words")
```


## Cloud of words 
The bigger the word, the more frequently it appears.

```{r echo=FALSE}
textplot_wordcloud(season.dfm)
```

## The 10 most frequent words per season
This plot doesn't tells us much information.

```{r echo=FALSE}
season.dfm %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~ season_scripts$season, ncol = 2) + 
  ggtitle("Top 10 TF per season")
```

## The TF-IDF per season

```{r}
season.tfidf %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~ season_scripts$season, ncol = 2) + 
    ggtitle("Top 10 TF-IDF per season")
```


# Exploratory Data analysis 
We see that "yeah", "apartment", "hey" "knock" and "gonna" are the most frequent terms as we could also see before. 
Terms such as "meemaw", "lesley", "missy", "arthur" and "bert" are rather more likely to be specific to a dew texts while still somehow frequent. 

```{r echo=FALSE}
season.freq %>% 
  ggplot(aes(x=log10(docfreq),
             y=log10(frequency))) + 
  geom_text(aes(label=feature),
            position=position_jitter(),
            size=3) + 
  xlab("Season log-frequency") + 
  ylab("log-frequency") + 
  ggtitle("Representation of the frequencies agains the  frequencies")
```

## Lexical diversity 
We compute the lexical diversity of the scripts. 

```{r echo=FALSE}
season.dfm %>% textstat_lexdiv()
```

```{r echo=FALSE}
season.dfm %>% textstat_lexdiv() %>%
  arrange(desc(TTR)) %>%
  slice(1:10) %>% #plot only the top 10
  ggplot(aes(reorder(document, -TTR),
             TTR)) + 
  geom_bar(stat = "identity") + 
  xlab("Seasons") + 
  ggtitle("Plot of the 10 top Token-Type-Ratio") 
```

## Keyness analysis 

```{r echo=FALSE}
## text5
season.keyness <- textstat_keyness(season.dfm, target = "text5")

textplot_keyness(season.keyness)

## text9
season.keyness <- textstat_keyness(season.dfm, target = "text9")

textplot_keyness(season.keyness)
```


```{r echo=FALSE}
#Table for co-occurance for all features
season.co <- fcm(season.tk, 
                context = "document", 
                tri = FALSE,
                )
season.co
```


```{r}
#Table for co-occurance for features with more than 300 co-occurances
index <- season.freq %>% 
  filter(frequency > 300) %>% 
  data.frame() %>% 
  select(feature)
season.co <- season.co[index$feature, index$feature]
season.co
```


```{r echo=FALSE}
#Co-occurence graph with every line being 400 occurances. I believe we can do some more data cleaning, as these words take
#a lot of occurances, while it problably wont help for our sentiment analysis
season.co[season.co <= 400] <- 0
season.co[season.co > 400] <- 1
network <- graph_from_adjacency_matrix(
  season.co,
  mode = "undirected",
  diag = FALSE)
plot(network,
     layout = layout_with_kk)
```