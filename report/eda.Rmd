---
output:
  html_document: default
  pdf_document: default
---
```{r, echo = FALSE, message = FALSE, warning = FALSE}
source("../scripts/setup.R")
```


```{r echo=FALSE}
data_scripts <- read.csv("../data/series_scripts.csv")
season_scripts <- read.csv("../data/season_scripts.csv")
```


# Tokenization and cleaning of the data 
We define the corpus for our analysis. The corpus will be the scripts column in our data set as it contains all the texts that must be analysed. Once defined, we clean the texts by removing the numbers, the punctuation, the symbols, and the separators. 

In our scripts, we can see the main characters that are _Sheldon_, _Leonard_, _Penny_, _Howard_ and _Raj_. They appear in each seasons' episodes. Therefore, we remove their name for the following explanatory data analysis because it will otherwise bias our results. 

We noticed that the term _'Amy'_ and _'Bernadette'_ appears the most. These are characters that appear towards the end of the series. The term 'scene' is also normal that it appears a lot, because at the beginning of each script and every time characters change of scene, the term 'scene' is written.
Therefore, we also remove these terms for the following analysis. 


```{r echo=FALSE}
season.cp <- corpus(season_scripts$agg_script)

season.tk <- tokens(
  season.cp,
  remove_numbers = TRUE, 
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE
  )

#Remove possessive apostrophe ('s), such that words as Leonard's are not skipped in the next part of the code
season.tk <- tokens_replace(
    season.tk, 
    types(season.tk), 
    stringi::stri_replace_all_regex(types(season.tk), "['\\p{Pf}][s]", "")
)

characters_names <- c("sheldon","leonard","penny","howard","raj", "amy", 
                      "bernadette", "emily", "cooper", "stuart", "lucy",
                      "arthur","rostenkowski","bert","mike","james","lorvis",
                      "dan","dr","beverly","isabella","williams","lesley",
                      "missy","dave","claire","meemaw", "mary","leslie","lalita",
                      "dennis","alfred", "susan", "ramona", "christie","gallo",
                      "zack", "wil", "kurt","toby", "amelia","nathan",
                      "beverley","haley","mandy","adam","barry","mark")

words_to_remove <- c("scene", "series", "episode","pilot", "yeah","uh","hey",
                    "gonna","knock","dr","apartment","um","wanna","door")

season.tk <- season.tk %>% 
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c(characters_names, words_to_remove)) 
```

# The Document-Term Matrix, the TF-IDF and the global frequencies 

```{r echo=FALSE}
## Compute the DTM
season.dfm <- dfm(season.tk)

## Compute the TF-IDF 
season.tfidf <- dfm_tfidf(season.dfm)  

## Compute the global frequencies
season.freq <- textstat_frequency(season.dfm)

## We take the top 40 terms that appear the most
season.freq %>% 
  top_n(40, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term") + 
  ggtitle("Top 40 most frequent words")
```


## Cloud of words 
The bigger the word, the more frequently it appears. The most frequent words seems to be "time", "fun", "love", "happy", etc. Does this means that the seasons have mostly positive sentiments ? We will analyse that throughout our report. 

```{r echo=FALSE}
textplot_wordcloud(season.dfm)
```

## The 10 most frequent words per season
Here, we plot the 10 most frequent words per season. We see that this plot doesn't tells us much information.

```{r echo=FALSE}
season.dfm %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~ season_scripts$season, ncol = 2) + 
  ggtitle("Top 10 TF per season")
```

## The TF-IDF per season
Again, here we plot the 10 most specific words per season. In the season 1 the term 'baby' is very specific. While in the season 4 the most specific term is 'birthday'.

```{r}
season.tfidf %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~ season_scripts$season, ncol = 2) + 
  ggtitle("Top 10 TF-IDF per season")
```


# Exploratory Data analysis 
On this plot, we cannot see very clearly as there are a lot of terms. But we see that "time", "guys" are indeed very frequent as seen previously and that "gablehouser" is very specific to one particular season. It is season 3 if we look at the previous plot.


```{r echo=FALSE}
season.freq %>% 
  ggplot(aes(x=log10(docfreq),
             y=log10(frequency))) + 
  geom_text(aes(label=feature),
            position=position_jitter(),
            size=3) + 
  xlab("Season log-frequency") + 
  ylab("log-frequency") + 
  ggtitle("Representation of the frequencies agains the  frequencies")
```

## Lexical diversity 
We compute the lexical diversity of the scripts and we see that the season do not have very diverse lexic. 

```{r echo=FALSE}
season.dfm %>% textstat_lexdiv()
```

```{r echo=FALSE}
season.dfm %>% textstat_lexdiv() %>%
  arrange(desc(TTR)) %>%
  slice(1:10) %>% #plot only the top 10
  ggplot(aes(reorder(document, -TTR),
             TTR)) + 
  geom_bar(stat = "identity") + 
  xlab("Seasons") + 
  ggtitle("Plot of the 10 top Token-Type-Ratio") 
```

## Keyness analysis 

```{r echo=FALSE}
## text5
season.keyness <- textstat_keyness(season.dfm, target = "text5")

textplot_keyness(season.keyness)

## text9
season.keyness <- textstat_keyness(season.dfm, target = "text9")

textplot_keyness(season.keyness)
```


```{r echo=FALSE}
#Table for co-occurance for all features
season.co <- fcm(season.tk, 
                context = "document", 
                tri = FALSE)
season.co
```


```{r}
#Table for co-occurrence for features with more than 300 co-occurrences
index <- season.freq %>% 
  filter(frequency > 300) %>% 
  data.frame() %>% 
  select(feature)
season.co <- season.co[index$feature, index$feature]
season.co
```


```{r echo=FALSE}
#Co-occurrence graph with every line being 400 occurrences. I believe we can do some more data cleaning, as these words take
#a lot of occurrences, while it problably wont help for our sentiment analysis
season.co[season.co <= 400] <- 0
season.co[season.co > 400] <- 1
network <- graph_from_adjacency_matrix(
  season.co,
  mode = "undirected",
  diag = FALSE)
plot(network,
     layout = layout_with_kk)
```
