---
output:
  html_document: default
  pdf_document: default
---
```{r, echo = FALSE, message = FALSE, warning = FALSE}
source("../scripts/setup.R")
```
### Part 2 : Exploratory Data Analysis (EDA)

```{r echo=FALSE}
data_scripts <- read.csv("../data/series_scripts.csv")
season_scripts <- read.csv("../data/season_scripts.csv")
```


# Tokenization and cleaning of the data 
We define the corpus for our analysis. The corpus will be the scripts column in our data set as it contains all the texts that must be analysed. Once defined, we clean the texts by removing the numbers, the punctuation, the symbols, and the separators. 

In our scripts, we can see the main characters that are _Sheldon_, _Leonard_, _Penny_, _Howard_ and _Raj_. They appear in each seasons' episodes. Therefore, we remove their name for the following explanatory data analysis because it will otherwise bias our results. 

We have created a variable _'characters_names'_ grouping the names of all the characters recurring during the series because these are, logically, the words that come up most often and this is not the purpose of our analysis here. It is the same with the words grouped in the variables _'words_to_remove'_, which are themselves linked to the stage directions or do not bring any added value. 

## OK if remove this paragraph ? 
(((We noticed that the term _'Amy'_ and _'Bernadette'_ appears the most. These are characters that appear towards the end of the series. The term 'scene' is also normal that it appears a lot, because at the beginning of each script and every time characters change of scene, the term 'scene' is written.
Therefore, we also remove these terms for the following analysis.)))


```{r echo=FALSE}
season.cp <- corpus(season_scripts$agg_script)

season.tk <- tokens(
  season.cp,
  remove_numbers = TRUE, 
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE)


#Remove possessive apostrophe ('s), such that words as Leonard's are not skipped in the next part of the code
season.tk <- tokens_replace(
    season.tk, 
    types(season.tk),
    stringi::stri_replace_all_regex(types(season.tk), "['\\p{Pf}][s]", ""))

season.tk <- tokens_replace(season.tk,
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)


characters_names <- c("sheldon","leonard","penny","howard","raj", "amy", 
                      "bernadette", "emily", "cooper", "stuart", "lucy",
                      "arthur","rostenkowski","bert","mike","james","lorvis",
                      "dan","dr","beverly","isabella","williams","lesley",
                      "missy","dave","claire","meemaw", "mary","leslie","lalita",
                      "dennis","alfred", "susan", "ramona", "christie","gallo",
                      "zack", "wil", "kurt","toby", "amelia","nathan",
                      "beverley","haley","mandy","adam","barry","mark","rebecca","randall",
                      "colonel","priya","Elizabeth","Katee","halley","alice","stan","valentine",
                      "jimmy","wyatt","kim","koothrapalli")

words_to_remove <- c("scene", "series", "episode","pilot", "yeah","uh","hey",
                    "gonna","knock","dr","apartment","um","wanna","door","Time shift","corridor")

season.tk <- season.tk %>% 
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c(characters_names, words_to_remove)) 

```

# The Document-Term Matrix, the TF-IDF and the global frequencies 

We use the TF-IDF method to look at the specificity of the terms through seasons. It allocates a weight to the most frequent words and translate a relevance of terms in the corpus. With the following graph, we can see that the most frequent term, is by far _'Time'_. Indeed with the frequency matrix, we can see that _'Time'_ appears more than 900 times in the whole show. 

```{r echo=FALSE}
## Compute the DTM
season.dfm <- dfm(season.tk)

## Compute the TF-IDF 
season.tfidf <- dfm_tfidf(season.dfm)  

## Compute the global frequencies
season.freq <- textstat_frequency(season.dfm)

## We take the top 40 terms that appear the most
season.freq %>% 
  top_n(40, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term") + 
  ggtitle("Top 40 most frequent words")
```
We observe that the most frequent terms are part of the feeling lexical field. For instance the words _'feel'_, _'love'_, _'fine'_. With only this first representation, we can observe a positive pattern in the show. 

## Cloud of words 
The bigger the word, the more frequently it appears. The most frequent words seems to be "time", "guy", "love", "feel", etc. Does this means that the seasons have mostly positive sentiments ? We will analyse that throughout our report. 

```{r echo=FALSE}
textplot_wordcloud(season.dfm)
```

## The 10 most frequent words per season
Here, we plot the 10 most frequent words per season. It is not really relevant because _'time'_ and _'guy'_ are always predominant through all the corpus. It looks logical to have them appearing individually.

```{r echo=FALSE}
season.dfm %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~ season_scripts$season, ncol = 2) + 
  ggtitle("Top 10 TF per season")
```

## The TF-IDF per season
Again, here we plot the 10 most specific words per season. In the season 1 the term 'feel' is very specific. While in the season 4 the most specific term is 'alright'.If we look at season 10 the name Hailey is very specific to this season, we can guess that this character is very important in the final story telling. 

## Does not work
```{r echo=FALSE}
season.tfidf %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~ season_scripts$season, ncol = 2) + 
  ggtitle("Top 10 TF-IDF per season")
```
# Exploratory Data analysis 
On this plot, we cannot see very clearly as there are a lot of terms. But we see that "time", "guys" are indeed very frequent as seen previously and that "gablehouser" is very specific to one particular season. It is season 1 and 2 with research. In the show it is actually a character Dr. Eric Gablehauser, he does not appear in the show after the 2 first season. 

```{r echo=FALSE}
season.freq %>% 
  ggplot(aes(x=log10(docfreq),
             y=log10(frequency))) + 
  geom_text(aes(label=feature),
            position=position_jitter(),
            size=3) + 
  xlab("Season log-frequency") + 
  ylab("log-frequency") + 
  ggtitle("Representation of the frequencies agains the  frequencies")
```

## Lexical diversity 
We compute the lexical diversity of the scripts and we see that the season, especially the last (7,8,9,10) do not have very diverse lexical. Indeed the TTR is equal to 0,25, which means that in a sequence of 4 words, 3 words are the same and only 1 is different. 

```{r echo=FALSE}
season.dfm %>% textstat_lexdiv()
```

```{r echo=FALSE}
season.dfm %>% textstat_lexdiv() %>%
  arrange(desc(TTR)) %>%
  slice(1:10) %>% #plot only the top 10
  ggplot(aes(reorder(document, -TTR),
             TTR)) + 
  geom_bar(stat = "identity") + 
  xlab("Seasons") + 
  ggtitle("Plot of the 10 top Token-Type-Ratio") 
```
Here, season 2 has the richest diversity, with a score of almost 0,5. 

## Keyness analysis 

We made a Keyness analysis, to understand what is the ration of the terms in the target compared to others in the rest of the corpus. 

- In the first graph, we analysis season 5 as the target. The reference is the rest of the corpus, and we observe that the word _'jimmy'_ is the most used in the script of season 5 compared to the rest. 

- In the second graph, we analysis season 7 as the target. The reference is the rest of the corpus, and we observe that the word _'element'_ is the most used in the script of season 7 compared to the rest. 

```{r echo=FALSE}
## text5
season.keyness <- textstat_keyness(season.dfm, target = "text5")

textplot_keyness(season.keyness)

## text7
season.keyness <- textstat_keyness(season.dfm, target = "text7")

textplot_keyness(season.keyness)
```


## Co-occurence analysis 
Next, we decided to create a co-occurence matrix to have an overview on which word often appears together. 
```{r echo=FALSE, include=FALSE}
#Table for co-occurance for all features
season.co <- fcm(season.tk, 
                context = "document", 
                tri = FALSE)
season.co
```


Because it is very difficult to see when we have too many words, we decided to restrict the co-occurence matrix to terms that co-occur more than 300 times together.

```{r}
#Table for co-occurrence for features with more than 300 co-occurrences
index <- season.freq %>% 
  filter(frequency > 300) %>% 
  data.frame() %>% 
  select(feature)
season.co <- season.co[index$feature, index$feature]
season.co
```
Here is a representation of how many times the most frequent words co-occur together in all the corpus. For example, _'time'_ and _'guy'_ co-occur 99'510 times together in the corpus, which means that they are used in the same context in the script. 

Below, we plot a co-occurrence graph. Each connection means that the two words appear together more than 400 times. We cannot see really well as many links are made.

## Is this graph really meaningful ?
```{r echo=FALSE}
#Co-occurrence graph with every line being 400 occurrences. I believe we can do some more data cleaning, as these words take
#a lot of occurrences, while it problably wont help for our sentiment analysis
season.co[season.co <= 400] <- 0
season.co[season.co > 400] <- 1
network <- graph_from_adjacency_matrix(
  season.co,
  mode = "undirected",
  diag = FALSE)
plot(network,
     layout = layout_with_kk)
```
