---
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```

```{r}
data_scripts <- read.csv("../data/series_scripts.csv")

season_scripts <- read.csv("../data/season_scripts.csv")
```



### Sentiment Analysis

## 1.Dictionary-based analysis
```{r}
library(tidyverse)
library(tidytext)
library(readr)
library(sentimentr)
library(quanteda)
library(quanteda.textstats)
library(lexicon) 
library(flextable)
```
# NRC dictionary
```{r}
get_sentiments(lexicon = "nrc")
```

We join the corresponding sentiment qualifier in “nrc” using inner.join() from dplyr:
```{r}
script.tb <- as_tibble(
  data.frame(
    data_scripts))

script.tok <- unnest_tokens(
  script.tb,
  output = "word",
  input = "script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)

script.sent <- 
  inner_join(
    script.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))
head(script.sent, 10) %>% flextable() %>% autofit()
```

Summary (long format)
```{r}
table(script.sent$document , 
      script.sent$sentiment)

## Long format + barplots
script.sent %>% 
  group_by(document, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ document) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):
```{r}
#totals by document
script.sent.doc.total <- 
  script.sent %>% 
  group_by(document) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  script.sent,
  script.sent.doc.total
) %>% 
  group_by(document, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ document) + 
  coord_flip()
```


## 2. Value-Based analysis ("afinn" dictionary)
```{r}
get_sentiments("afinn")
```

```{r}
script.sent <-
  inner_join(
    script.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
script.sent %>% 
  group_by(document) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r}
script.sent %>% 
  group_by(document) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(document, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```
## Quanteda analysis
```{r}
summary(script.cp)

tokens_lookup(
  script.tk,
  dictionary = data_dictionary_LSD2015
  ) 

script.sent1 <- tokens_lookup(
  script.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()
script.sent1 %>% 
  pivot_wider(
    names_from = "term",
    values_from = "count"
  ) %>% 
  mutate(negative = replace_na(negative, 0),
         Score = round(negative / (negative + positive), 3)) %>% 
  arrange(Score) %>% 
  head(20) %>% 
  flextable() %>% 
  autofit() 
```

```{r}
ggplot(script.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```

## Valence-Shifters analysis
```{r}
library(sentimentr)
library(lexicon)
```

```{r}
hash_sentiment_jockers_rinker
hash_valence_shifters
```

```{r}
# Very long time to execute
#mytext <- get_sentences(data_scripts$script)
#mytext
```


```{r}
# Same
#sentiment(mytext) %>% 
  #flextable() %>% 
  #autofit()
```


#################################### Season Analysis ##################################################

### Sentiment Analysis

## 1.Dictionary-based analysis
```{r}
library(tidyverse)
library(tidytext)
library(readr)
library(sentimentr)
library(quanteda)
library(quanteda.textstats)
library(lexicon) 
library(flextable)
```
# NRC dictionary
```{r}
get_sentiments(lexicon = "nrc")
```

We join the corresponding sentiment qualifier in “nrc” using inner.join() from dplyr:
```{r}
season.tb <- as_tibble(
  data.frame(
    season_scripts))

season.tok <- unnest_tokens(
  season.tb,
  output = "word",
  input = "agg_script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)

season.sent <- 
  inner_join(
    season.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))
head(season.sent, 10) %>% flextable() %>% autofit()
```

Summary (long format)
```{r}
table(season.sent$season, 
      season.sent$sentiment)

## Long format + barplots --> NOT WORKING
season.sent %>% 
  group_by(season, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ season_scripts$season ) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):
```{r}
#totals by document
season.sent.doc.total <- 
  season.sent %>% 
  group_by(season) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  season.sent,
  season.sent.doc.total
  ) %>% 
  group_by(season, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ season_scripts$season) + 
  coord_flip()
```


## 2. Value-Based analysis ("afinn" dictionary)
```{r}
get_sentiments("afinn")
```

```{r}
season.sent <-
  inner_join(
    season.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r}
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(season, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```
## Quanteda analysis
```{r}
summary(season.cp)

tokens_lookup(
  season.tk,
  dictionary = data_dictionary_LSD2015
  ) 

season.sent1 <- tokens_lookup(
  season.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()
season.sent1 %>% 
  pivot_wider(
    names_from = "term",
    values_from = "count"
  ) %>% 
  mutate(negative = replace_na(negative, 0),
         Score = round(negative / (negative + positive), 3)) %>% 
  arrange(Score) %>% 
  head(20) %>% 
  flextable() %>% 
  autofit() 
```

```{r}
ggplot(season.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```

## Valence-Shifters analysis
```{r}
library(sentimentr)
library(lexicon)
```

```{r}
hash_sentiment_jockers_rinker
hash_valence_shifters
```

```{r}
# Very long time to execute
#mytext <- get_sentences(season_scripts$agg_script)
#mytext
```


```{r}
# Same
#sentiment(mytext) %>% 
  #flextable() %>% 
  #autofit()
```



