---
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```

```{r}
data_scripts <- read.csv("../data/series_scripts.csv")
season_scripts <- read.csv("../data/season_scripts.csv")
```



### Sentiment Analysis on the episodes 

#### 1.Dictionary-based analysis

##### NRC dictionary

```{r include=False}
get_sentiments(lexicon = "nrc")
```

We join the corresponding sentiment qualifier in “nrc” using inner.join() from dplyr:

```{r echo=FALSE}
script.tb <- as_tibble(
  data.frame(
    data_scripts))

script.tok <- unnest_tokens(
  script.tb,
  output = "word",
  input = "script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)

script.sent <- 
  inner_join(
    script.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))
head(script.sent, 10) %>% flextable() %>% autofit()
```

Summary (long format)

```{r echo=FALSE}
## Long format + barplots
script.sent %>% 
  group_by(document, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ document) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):

```{r echo=FALSE}
#totals by document
script.sent.doc.total <- 
  script.sent %>% 
  group_by(document) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  script.sent,
  script.sent.doc.total
) %>% 
  group_by(document, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ document) + 
  coord_flip()
```


#### 2. Value-Based analysis ("afinn" dictionary)
```{r include=False}
get_sentiments("afinn")
```

```{r echo=FALSE}
script.sent <-
  inner_join(
    script.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
script.sent %>% 
  group_by(document) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r echo=FALSE}
script.sent %>% 
  group_by(document) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(document, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```

### Quanteda analysis
```{r echo=FALSE}
summary(script.cp)

tokens_lookup(
  script.tk,
  dictionary = data_dictionary_LSD2015
  ) 

script.sent1 <- tokens_lookup(
  script.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()
script.sent1 %>% 
  pivot_wider(
    names_from = "term",
    values_from = "count"
  ) %>% 
  mutate(negative = replace_na(negative, 0),
         Score = round(negative / (negative + positive), 3)) %>% 
  arrange(Score) %>% 
  head(20) %>% 
  flextable() %>% 
  autofit() 
```

```{r echo=FALSE}
ggplot(script.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```

### Valence-Shifters analysis

```{r include=False}
hash_sentiment_jockers_rinker
hash_valence_shifters
```

```{r}
# Very long time to execute
#mytext <- get_sentences(data_scripts$script)
#mytext
```


```{r}
# Same
#sentiment(mytext) %>% 
  #flextable() %>% 
  #autofit()
```


#################################### Season Analysis ##################################################

### Sentiment Analysis on the seasons 

#### 1.Dictionary-based analysis

##### NRC dictionary
We decided to first use the NRC dictionary. The NRC dictionary is an emotion lexicon. It is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).  
(https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

```{r include=False}
get_sentiments(lexicon = "nrc")
```

We join the corresponding sentiment qualifier in “nrc” using inner.join() from dplyr:

```{r include=FALSE}
season.tb <- as_tibble(
  data.frame(
    season_scripts))

season.tok <- unnest_tokens(
  season.tb,
  output = "word",
  input = "agg_script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)

season.sent <- 
  inner_join(
    season.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))
head(season.sent, 10) %>% flextable() %>% autofit()
```

Summary (long format)

```{r echo=FALSE}
table(season.sent$season, 
      season.sent$sentiment)

## Long format + barplots --> NOT WORKING
season.sent %>% 
  group_by(season, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ season ) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):
```{r echo=FALSE}
#totals by document
season.sent.doc.total <- 
  season.sent %>% 
  group_by(season) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  season.sent,
  season.sent.doc.total
  ) %>% 
  group_by(season, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ season) + 
  coord_flip()
```


#### 2. Value-Based analysis ("afinn" dictionary)
Now, we also want to try the "afinn" dictionary. This dictionary contains 2,477 coded words associated to a score range from minus five (negative) to plus five (positive). 

```{r include=FALSE}
get_sentiments("afinn")
```

```{r echo=FALSE}
season.sent <-
  inner_join(
    season.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r echo=FALSE}
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(season, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```

### Quanteda analysis

```{r echo=FALSE}
season.tk <- tokens(
  season.cp,
  remove_numbers = TRUE, 
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE
  )

#Remove possessive apostrophe ('s), such that words as leonard's are not skipped in the next part of the code
season.tk <- tokens_replace(
    season.tk, 
    types(season.tk), 
    stringi::stri_replace_all_regex(types(season.tk), "['\\p{Pf}][s]", "")
)

season.tk <- season.tk %>% 
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(
    c("sheldon","leonard","penny","howard",
      "raj", "amy", "bernadette", "scene", "series", "episode"))  #remove main characters'name and 'scene'
```


```{r echo=FALSE}
season.cp <- corpus(season_scripts$agg_script)
summary(season.cp)

tokens_lookup(
  season.tk,
  dictionary = data_dictionary_LSD2015
  ) 

season.sent1 <- tokens_lookup(
  season.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()
season.sent1 %>% 
  pivot_wider(
    names_from = "term",
    values_from = "count"
  ) %>% 
  mutate(negative = replace_na(negative, 0),
         Score = round(negative / (negative + positive), 3)) %>% 
  arrange(Score) %>% 
  head(20) %>% 
  flextable() %>% 
  autofit() 
```

```{r echo=FALSE}
ggplot(season.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```

#### Valence-Shifters analysis

```{r include=FALSE}
hash_sentiment_jockers_rinker
hash_valence_shifters
```

```{r}
# Very long time to execute because it extracts the sentences out of the scripts
#mytext <- get_sentences(season_scripts$agg_script)
#mytext
```


```{r}
# Same
# sentiment(mytext) %>% 
#   flextable() %>% 
#   autofit()
```



