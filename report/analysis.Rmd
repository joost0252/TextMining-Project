---
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```

```{r echo=FALSE}
data_scripts <- read.csv("../data/series_scripts.csv")
season_scripts <- read.csv("../data/season_scripts.csv")
```

### Sentiment Analysis
In this following part, we want to compute the sentiment of each season. To do so, we decide to first used the 'NRC' dictionary to perform the analysis. Then we want to compare our results to another analysis using another dictionary, the 'afinn' dictionary. 


#### NRC dictionary
The NRC dictionary contains a list of English words and their associations with eight basic emotions and two sentiments (positive or negative). These emotions are anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. Based on this dictionary, one English word can be associated to several emotions. For example, we see on the following table that the term 'abandon' is associated to several emotions (fear, negative, sadness). 

```{r echo=FALSE}
head(get_sentiments(lexicon = "nrc"))
```


For each token in our data scripts, we join the corresponding sentiment qualifier in “nrc” using the `inner.join()` function from `dplyr`:

```{r}
season.tb <- as_tibble(data.frame(season_scripts))

words_to_remove <- c("scene", "series", "episode", "pilot", "yeah", "uh", "hey",
                    "gonna", "knock", "dr", "apartment", "um", "wanna", "door")

season.tok <- unnest_tokens(
  season.tb,
  output = "word",
  input = "agg_script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)
 
season.tok <- season.tok %>%
  anti_join(stop_words, by = c("word" = "word"))
  
season.sent <- 
  inner_join(
    season.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))

head(season.sent, 10) %>% 
  flextable() %>%
  autofit()
```

Here, we show the overall sentiment per season.

```{r echo=FALSE}
table(season.sent$season, 
      season.sent$sentiment)

## Long format
season.sent %>% 
  group_by(season, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ season ) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):
```{r}
#totals by document
season.sent.doc.total <- 
  season.sent %>% 
  group_by(season) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  season.sent,
  season.sent.doc.total
  ) %>% 
  group_by(season, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ season) + 
  coord_flip()
```


#### AFINN dictionary
Now, on this part, we will us the AFINN dictionary. This dictionary contains a list of English words manually rated for valence with an integer between -5 (very negative) and +5 (very positive) by Finn Årup Nielsen.


We see on the below table that the word 'abandoned' has a value of -2 which means that it is relatively negative. 

```{r echo=FALSE}
head(get_sentiments("afinn"))
```

We see there on the table below that the seasons 3, 4 and 5 are categorized as relatively negative with season 3 being the most negative. 

```{r echo=FALSE}
season.sent <-
  inner_join(
    season.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r}
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(season, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```

## Quanteda analysis
```{r}
season.cp <- corpus(season_scripts$agg_script)
summary(season.cp)

season.tk <- tokens(
  season.cp, 
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE)
season.tk <- tokens_tolower(season.tk) 
season.tk <- tokens_replace(
  season.tk,
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)
season.tk <- season.tk %>% 
  tokens_remove(words_to_remove)
#season.tk

tokens_lookup(
  season.tk,
  dictionary = data_dictionary_LSD2015
  ) 

# If you get an error with the below code, this is how I fixed it: 
# 1. Open the registry edit tool by searching regedit in the start menu
# 2. Navigate to Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem
# 3. Change LongPathsEnabled from 0 to 1
season.sent1 <- tokens_lookup(
  season.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()
season.sent1 %>% 
  pivot_wider(
    names_from = "term",
    values_from = "count"
  ) %>% 
  mutate(negative = replace_na(negative, 0),
         Score = round(negative / (negative + positive), 3)) %>% 
  arrange(Score) %>% 
  head(20) %>% 
  flextable() %>% 
  autofit() 
```

```{r}
ggplot(season.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```

## Valence-Shifters analysis

```{r}
hash_sentiment_jockers_rinker
hash_valence_shifters
```

```{r}
# Very long time to execute
#mytext <- get_sentences(season_scripts$agg_script)
#mytext
```


```{r}
# Same
#sentiment(mytext) %>% 
  #flextable() %>% 
  #autofit()
```

### Analysis by character --> Doesn't work

```{r}
#tidy_nrc <- tidy_text %>% inner_join(nrc)
season.sent %>% 
  filter(value %in% c("sheldon","leonard","penny","howard","raj")) %>% 
  ggplot(aes(sentiment, fill = "black"))+
  geom_bar(show.legend = FALSE)+
  facet_wrap(~c("sheldon","leonard","penny","howard","raj"), scales="free_y")+
  theme_dark()+
  theme(
    strip.text = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
  )+
  labs(fill = NULL, x = NULL, y = "Sentiment Frequency", title = "Sentiments of each characters by using nrc lexicon")+
  scale_fill_manual(values = c("#EA181E", "#00B4E8", "#FABE0F","#EA181E", "#00B4E8", "#FABE0F"))+
  annotation_custom(grid::roundrectGrob(), ymax = 4000, ymin = 2000, xmin = 1, xmax = 5)
```

## Similarities between season scripts
```{r}
season.dfm <- dfm(season.tk)
season.tfidf <- dfm_tfidf(season.dfm)

season.jac <- textstat_simil(
  season.tfidf,
  method = "jaccard",
  margin = "documents")

season.cos <- textstat_simil(
  season.tfidf,
  method = "cosine",
  margin = "documents")

season.euc <- textstat_dist(
  season.tfidf,
  method = "euclidean",
  margin = "documents")
```

```{r}
## Jaccard 
season.jac.mat <- melt(as.matrix(season.jac)) # Convert the object to matrix then to data frame 
ggplot(data = season.jac.mat, 
       mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white", 
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Jaccard") +
  geom_tile() + xlab("") + ylab("")

## Cosine
season.cos.mat <- melt(as.matrix(season.cos))
ggplot(
  data = season.cos.mat,
  mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + xlab("") + ylab("")

## Euclidean
season.euc.mat <- melt(as.matrix(season.euc))
M <- max(season.euc.mat$value) # maximum distance
season.euc.mat$value.std <- (M - season.euc.mat$value)/M 
# conversion from distance to similarity in [0,1]
ggplot(
  data = season.euc.mat,
  mapping = aes(x = Var1, 
                y = Var2,
                fill = value.std)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white", 
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("")
```

## Clustering 
```{r}
season.hc <- hclust(as.dist(season.euc))
## crude.hc <- hclust(as.dist(1 - crude.jac)) # use this line for Jaccard
## crude.hc <- hclust(as.dist(1 - crude.cos)) # use this line for Cosine
plot(season.hc)

season.clust <- cutree(season.hc, k = 3)
season.clust

season.km <- kmeans(season.tfidf, centers = 3)
season.km$cluster


data.frame(
  Clust.1 = names(sort(apply(season.tfidf[season.clust==1, ], 2, sum), decreasing = TRUE)[1:5]),
  Clust.2 = names(sort(apply(season.tfidf[season.clust==2, ], 2, sum), decreasing = TRUE)[1:5]),
  Clust.3 = names(sort(apply(season.tfidf[season.clust==3, ], 2, sum), decreasing = TRUE)[1:5])
)


data.frame(
  Clust.1 = names(sort(apply(season.tfidf[season.km$cluster==1,], 2, sum), decreasing = TRUE)[1:5]),
  Clust.2 = names(sort(apply(season.tfidf[season.km$cluster==2,], 2, sum), decreasing = TRUE)[1:5]),
  Clust.3 = names(sort(apply(season.tfidf[season.km$cluster==3,], 2, sum), decreasing = TRUE)[1:5])
)
```


## Similarities between words
```{r}
season.feat <- textstat_frequency(season.dfm) %>%
  filter(rank <= 40) 
season.feat$feature

season.cos <- textstat_simil(
  season.dfm[, season.feat$feature],
  method = "cosine",
  margin = "feature")
season.cos.mat <- melt(as.matrix(season.cos)) # Convert the object to matrix then to data frame 

ggplot(data = season.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
    axis.text.y = element_text(size = 5)) +
  xlab("") + 
  ylab("")

```

## Clustering words
```{r}
season.hcw <- hclust(as.dist(1 - season.cos))
plot(season.hcw)
```

## Co-occurences --> Doesn't work
```{r}
season.fcm <- fcm(season.tk, 
                 window = 3, 
                 tri = FALSE)
season.fcm <- (season.fcm + t(season.fcm))/2 ## make the co-occurrence matrix symmetrical

season.fcm.mat <- melt(
  as.matrix(
    season.fcm[season.feat$feature, season.feat$feature]),
  varnames = c("Var1", "Var2")) 
ggplot(data = season.fcm.mat, 
       mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 140,
    limit = c(0, 280),
    name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
        axis.text.y = element_text(size = 5)) +
  xlab("") +
  ylab("")
```

## Cluster dendrogram
```{r}
season.inv_occ <- 
  280 - as.matrix(
    season.fcm[season.feat$feature, season.feat$feature]) ## 280 is the max co-occurrence here
season.hc <- hclust(as.dist(season.inv_occ))
plot(season.hc)
```


## LSA on TF
```{r}
season.tf <- dfm(season.tk)
season.lsa <- textmodel_lsa(x = season.tf, nd = 10) 
head(season.lsa$docs)
```

```{r}
head(season.lsa$features)
```

```{r}
season.freq <- ntoken(season.tk) # row-sum of the DTM. Are you convinced it is the document length?
data.frame(season.freq,
           dim1 = season.lsa$docs[, 1]) %>% 
  ggplot(aes(season.freq, dim1)) + 
  geom_point() + 
  xlab("Number of tokens") + 
  ylab("LSA dim. 1")
```

```{r}
n.terms <- 5
## For Dimension 2
w.order <- sort(season.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
## For Dimension 3
w.order <- sort(season.lsa$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

```

```{r}
w.top2
```

```{r}
w.top3
```

```{r}
biplot(
  y = season.lsa$docs[, 2:3],
  x = season.lsa$features[, 2:3],
  col = c("black", "red"),
  cex = c(0.3, 0.3),
  xlab = "Dim 2",
  ylab = "Dim 3")
```

```{r}
w.subset <- 
  season.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]
biplot(
  y = season.lsa$docs[, 2:3],
  x = w.subset,
  col = c("black","red"),
  cex = c(0.5, 0.5),
  xlab = "Dim 2",
  ylab = "Dim 3")
```


## LSA on TF_IDF ?? Do we want to do that ?
```{r}

```

## LDA using quanteda
```{r}
set.seed(1234) #To create reproducible results
season.lda <- textmodel_lda(x = season.tf, k = 10)
seededlda::terms(season.lda, 5)
```

```{r}
seededlda::topics(season.lda)
```

```{r}
seededlda::topics(season.lda) %>% table()
```

## Term-Topic Analysis
```{r}
phi.long <- melt(
  season.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```


## Topic-Document Analysis
```{r}
set.seed(1234)
theta.long <- melt(
  season.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

```{r}
## 10 longest documents
## ntoken compute the lengths. They are sorted in decreasing order. 
## We take the 12 first
n.doc <- 12
doc.list <- names(
  sort(ntoken(season.tk), decreasing = TRUE)[1:n.doc])
doc.list
##  [1] "1841-Harrison" "1909-Taft"     "1845-Polk"     "1889-Harrison"
##  [5] "1821-Monroe"   "1837-VanBuren" "1897-McKinley" "1925-Coolidge"
##  [9] "1929-Hoover"   "1921-Harding"  "1853-Pierce"   "1861-Lincoln"

theta.long %>% 
  filter(Doc %in% doc.list) %>%  
  ggplot(aes(reorder_within(Topic, Theta, Doc), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Doc, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") +
   theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```


## LDA diagnostics
```{r}
rev(sort(colSums(season.lda$theta)/sum(season.lda$theta)))
```

```{r}
season.codo <- fcm(
  season.tf, 
  context = "document",
  count = "boolean",
  tri = FALSE) # co-document frequencies
term.mat <- seededlda::terms(season.lda, 5)
Coh <- rep(0, 10)
names(Coh) <- paste0("Topic", 1:10)
for (k in 1:10) {
  D.mat <- t(season.codo[term.mat[,k], term.mat[,k]])
  D.vec <- season.tf %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[, k]) %>% 
    data.frame %>%
    select(feature, docfreq)
  for (m in 2:5){
    for (l in 1:(m - 1)) {
      vm <- term.mat[m, k]
      vl <- term.mat[l, k]
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
    }
  }
}
rev(sort(Coh))
```

```{r}
as.matrix(season.codo[term.mat[, 3], term.mat[, 3]])
```

```{r}
as.matrix(season.codo[term.mat[, 5], term.mat[, 5]])
```

```{r}
excl <- rep(0, 10)
names(excl) <- paste0("Topic", 1:10)
for (k in 1:10) {
  for (i in 1:length(term.mat[,k])) {
    term.phi <- filter(phi.long, Term == term.mat[i,k])
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic1")$Phi / sum(term.phi$Phi)
  }
  excl[k] <- excl[k] / length(term.mat[, k])
}
rev(sort(excl))
```
The most exclusive topic is Topic 1, with five top terms are more specific to it.


## LDA with Topic Model
```{r}
season.LDA <- LDA(
  convert(season.tf, to = "topicmodels"), k = 10)
topicmodels::terms(season.LDA, 5)
```

```{r}
topicmodels::topics(season.LDA)
```

```{r}
topicmodels::topics(season.LDA) %>% table()
```

```{r}
topic_diagnostics(
  topic_model = season.LDA, 
  dtm_data = convert(season.tf, to = "topicmodels"))
```

```{r}
beta.long <- tidy(
  season.LDA,
  matrix = "beta") # equivalent to melt (with this package)

beta.long %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ggplot(aes(reorder_within(term, beta, topic), beta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme(
    axis.text.y = element_text(size = 5),
    axis.text.x = element_text(size = 5),
    strip.text = element_text(size = 5))
```
#Features: DTM and LSA
```{r}
# Get the script per character and tokenize this
names(season_scripts)
character.cp <- corpus(tweets, text_field = "text")## File with sentence of each sentence by character is needed.
character.tok ## Tokenize this corpus 

# The variable to predict is the character name:
y <- factor(docvars(character.tok, "character"))

#Compute the DTM matrix and show the dimensions:
character.dfm <- dfm(character.tok)
dim(character.dfm)
```

```{r}
#Using a LSA to reduce dimensions, the nd should be played with to try different results:
library(quanteda.textmodels)
character.lsa <- textmodel_lsa(character.dfm, nd=25)
head(character.lsa$docs) #In original file this was $docs, see what works.

```

```{r}
#Training the classifier:
character.df <- data.frame(Class=y, X=character.lsa$docs) #same here, $docs might need to be changed
index.tr <- sample(size=round(0.8*length(y)), x=c(1:length(y)), replace=FALSE)
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]
library(caret)
library(ranger)
train(df.tr) #This is used to estimate the parameters for the given model from a set of data
character.fit <- ranger(Class ~ ., 
                     data = df.tr)
pred.te <- predict(character.fit, df.te)

confusionMatrix(data=pred.te$predictions, reference = df.te$Class) #This gives the confusionmatrix, precision, specifity, and sensitivity
```
# Improving the features
```{r}

## Same as above, but now it tries different nd/number of dimensions, may take a long time
nd.vec <- c(2,5,25,50,100, 500, 1000)
acc.vec <- numeric(length(nd.vec))
for (j in 1:length(nd.vec)){
  character.lsa <- textmodel_lsa(character.dfm, nd=nd.vec[j])
  character.df <- data.frame(Class=y, X=character.lsa$docs)
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  
  character.fit <- ranger(Class ~ ., 
                       data = df.tr)
  pred.te <- predict(character.fit, df.te)
  acc.vec[j] <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)$overall[1]
}
## Growing trees.. Progress: 93%. Estimated remaining time: 2 seconds.
acc.vec
## [1] 0.7942547 0.8625776 0.8819876 0.8827640 0.8804348 0.8812112 0.8796584
plot(acc.vec ~ nd.vec, type='b')

```


## Word Embedding with glove
```{r}


library(text2vec)
character.fcm <- fcm(character.tok, 
           context = "window",
           count = "weighted",
           window=5,
           weights = 1/(1:5),
           tri = FALSE)
glove <- GlobalVectors$new(rank = 25, x_max = 1)

word_vectors_main <- glove$fit_transform(character.fcm, n_iter = 100)

```

```{r}

word_vectors_context <- glove$components
character.glove <- word_vectors_main + t(word_vectors_context)

ndoc <- length(character.tok) # number of documents
centers <- matrix(nr=ndoc, nc=25) # document embedding matrix (1 document per row)
for (i in 1:ndoc){
  words_in_i <- character.glove[character.tok[[i]],, drop=FALSE]
  centers[i,] <- apply(words_in_i,2,mean)
}
row.names(centers) <- names(character.tok)

character.df <- data.frame(Class=y, X=centers)
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]
character.fit <- ranger(Class ~ ., 
                     data = df.tr)
pred.te <- predict(character.fit, df.te)
confusionMatrix(data=pred.te$predictions, reference = df.te$Class)

```

# If we want to add more features (could be sentiment of sentence/word)
```{r}
character.tfidf <- dfm_tfidf(character.dfm)
character.lsa <- textmodel_lsa(character.tfidf, nd=25)

df <- data.frame(Class=y, X=tweets.lsa$docs)
character.df <- cbind(character.df, 
            logretweet=log10(docvars(character.tok, c("retweet_count"))),
            length = log(sapply(character.tok, length)))
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]
tweets.fit <- ranger(Class ~ ., 
                     data = df.tr, 
                     importance = "impurity")
pred.te <- predict(character.fit, df.te)
confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
```

#Further improvement based on adding the centroids (and the extra features from above if used)
```{r}

character.df <- cbind(character.df, Cent=centers) ## add the centroid features to the tf-idf-lsa, retweet, length features.
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]
character.fit <- ranger(Class ~ ., 
                     data = df.tr, 
                     importance = "impurity")
pred.te <- predict(character.fit, df.te)
confusionMatrix(data=pred.te$predictions, reference = df.te$Class)

```

####################################### Sentiment Analysis by Characters ###############################################

## Sentiments of each characters by using nrc lexicon 
```{r}
tidy_nrc %>% 
  filter(author %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  ggplot(aes(sentiment, fill = author))+
  geom_bar(show.legend = FALSE)+
  facet_wrap(author~.)+
  theme_dark()+
  theme(
    strip.text = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
  )+
  labs(fill = NULL, x = NULL, y = "Sentiment Frequency", title = "Sentiments of each characters by using nrc lexicon")+
  scale_fill_manual(values = c("#EA181E", "#00B4E8", "#FABE0F","#EA181E", "#00B4E8", "#FABE0F"))+
  annotation_custom(img,ymax = 4000, ymin = 2000, xmin = 1, xmax = 5)
```

## Negative-Positive Ratio in all seasons by using bing lexicon
```{r}
tidy_bing %>% 
  filter(author %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  group_by(season, author) %>% 
  count(sentiment) %>%
  ungroup() %>%
  ggplot(aes(season, n, fill = sentiment)) +
  geom_col(position = "fill") +
  geom_text(aes(label = n), position = position_fill(0.5), color = "white")+
  coord_flip()+
  facet_wrap(author~.)+
  theme_dark()+
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
    )+
  scale_fill_manual(values = c("#EA181E", "#00B4E8"))+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10))+
  labs(y = NULL,  x = "Season", fill = NULL, title = "Negative-Positive Ratio in all seasons by using bing lexicon")
```

## Total sentiment score
```{r}
tidy_afinn %>% 
  filter(author %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  group_by(season, author) %>% 
  summarise(total = sum(value), .groups = 'drop') %>% 
  ungroup() %>% 
  mutate(Neg = if_else(total < 0, TRUE, FALSE)) %>% 
  ggplot()+
  geom_path(aes(season, total, color = author), size = 1.2)+
  theme_minimal()+
  theme(legend.position = "bottom")+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10))+
  scale_color_manual(values = c("#EA181E", "#00B4E8", "#FABE0F", "seagreen2", "orchid", "royalblue"))+
  labs(x = "Season", color = NULL, y = "Total Sentiment Score")+
  annotation_custom(img, ymin = 350, ymax = 400, xmin = 1, xmax = 4)
```

```{r}

```


```{r}

```
