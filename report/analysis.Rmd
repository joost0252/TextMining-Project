---
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```

```{r echo=FALSE}
data_scripts <- read.csv("../data/series_scripts.csv")
season_scripts <- read.csv("../data/season_scripts.csv")
character <- read.csv("../data/character_speech.csv", stringsAsFactors = FALSE)
```

### Sentiment Analysis
In this following part, we want to compute the sentiment of each season. To do so, we decide to first used the 'NRC' dictionary to perform the analysis. Then we want to compare our results to another analysis using another dictionary, the 'afinn' dictionary. 


#### NRC dictionary
The NRC dictionary contains a list of English words and their associations with eight basic emotions and two sentiments (positive or negative). These emotions are anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. Based on this dictionary, one English word can be associated to several emotions. For example, we see on the following table that the term 'abandon' is associated to several emotions (fear, negative, sadness). 

```{r echo=FALSE, include=FALSE}
head(get_sentiments(lexicon = "nrc"))
```


For each token in our data scripts, we join the corresponding sentiment qualifier in “nrc” using the `inner.join()` function from `dplyr`:
Below, you can see the first 10 rows of the dictionary. 

```{r echo=FALSE}
season.tb <- as_tibble(data.frame(season_scripts))

words_to_remove <- c("scene", "series", "episode", "pilot", "yeah", "uh", "hey",
                    "gonna", "knock", "dr", "apartment", "um", "wanna", "door")

season.tok <- unnest_tokens(
  season.tb,
  output = "word",
  input = "agg_script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)
 
season.tok <- season.tok %>%
  anti_join(stop_words, by = c("word" = "word"))

season.tok <- season.tok %>% 
  filter(!word %in% words_to_remove) %>% 
  mutate(word = str_remove_all(word, "'s")) 
  
season.sent <- 
  inner_join(
    season.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))

head(season.sent, 10) %>% 
  flextable() %>%
  autofit()
```

Here, we show the overall sentiment per season. It seems that season 2 and 6 has very few sentiments.

```{r echo=FALSE}
table(season.sent$season, 
      season.sent$sentiment)

## Long format
season.sent %>% 
  group_by(season, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ season ) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):
```{r}
#totals by document
season.sent.doc.total <- 
  season.sent %>% 
  group_by(season) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  season.sent,
  season.sent.doc.total
  ) %>% 
  group_by(season, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ season) + 
  coord_flip()
```


By re-scaling, we see that all seasons follow the same pattern, meaning that they are mainly positive and reflecting the sentiment of trust while very few of disgust sentiments appear.

#### AFINN dictionary
Now, on this part, we will us the AFINN dictionary. This dictionary contains a list of English words manually rated for valence with an integer between -5 (very negative) and +5 (very positive) by Finn Årup Nielsen.


We see on the below table that the word 'abandoned' has a value of -2 which means that it is relatively negative. 

```{r echo=FALSE}
head(get_sentiments("afinn"))
```

We see there on the table below that the seasons 3, 2 and 7 are categorized as relatively negative with season 3 being the most negative. 

```{r echo=FALSE}
season.sent <-
  inner_join(
    season.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r echo=FALSE}
season.sent %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(season, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```

## Quanteda analysis
```{r echo=FALSE}
season.cp <- corpus(season_scripts$agg_script)
summary(season.cp)

season.tk <- tokens(
  season.cp, 
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE)

season.tk <- tokens_tolower(season.tk) 

season.tk <- tokens_replace(
  season.tk,
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)

season.tk <- season.tk %>% 
  tokens_remove(words_to_remove)
#season.tk

tokens_lookup(
  season.tk,
  dictionary = data_dictionary_LSD2015
  ) 


season.sent1 <- tokens_lookup(
  season.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()

#This part can be removed, it is repetitive
# season.sent1 %>% 
#   pivot_wider(
#     names_from = "term",
#     values_from = "count"
#   ) %>% 
#   mutate(negative = replace_na(negative, 0),
#          Score = round(negative / (negative + positive), 3)) %>% 
#   arrange(Score) %>% 
#   head(20) %>% 
#   flextable() %>% 
#   autofit() 
```

From another dictionary named 'data_dictionary_LSD2015', we see pretty much the same analysis. 

```{r}
ggplot(season.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```

## Valence-Shifters analysis

```{r echo=FALSE, include=FALSE}
hash_sentiment_jockers_rinker
hash_valence_shifters
```



## Similarities between season scripts
In this part, we want to compare the similarities of the scripts between the season. We decided to use the 3 similarities measure to compute the similarity matrix: Jaccard similarity, Cosine distance, and Euclidean distance. 

```{r echo=FALSE}
season.dfm <- dfm(season.tk)
season.tfidf <- dfm_tfidf(season.dfm)

season.jac <- textstat_simil(
  season.tfidf,
  method = "jaccard",
  margin = "documents")

season.cos <- textstat_simil(
  season.tfidf,
  method = "cosine",
  margin = "documents")

season.euc <- textstat_dist(
  season.tfidf,
  method = "euclidean",
  margin = "documents")
```

```{r echo=FALSE}
## Jaccard 
season.jac.mat <- melt(as.matrix(season.jac)) # Convert the object to matrix then to data frame 
ggplot(data = season.jac.mat, 
       mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white", 
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Jaccard") +
  geom_tile() + xlab("") + ylab("")

## Cosine
season.cos.mat <- melt(as.matrix(season.cos))
ggplot(
  data = season.cos.mat,
  mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + xlab("") + ylab("")

## Euclidean
season.euc.mat <- melt(as.matrix(season.euc))
M <- max(season.euc.mat$value) # maximum distance
season.euc.mat$value.std <- (M - season.euc.mat$value)/M 
# conversion from distance to similarity in [0,1]
ggplot(
  data = season.euc.mat,
  mapping = aes(x = Var1, 
                y = Var2,
                fill = value.std)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white", 
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("")
```

We get 3 quite different co-occurence matrices. From the Euclidean co-occurence plot, it seems that season 4 is quite close to every other seasons. 

## Clustering 
Then, to create a cluster, we decide to focus on the Euclidean distance only.

```{r}
season.hc <- hclust(as.dist(season.euc))
## crude.hc <- hclust(as.dist(1 - crude.jac)) # use this line for Jaccard
## crude.hc <- hclust(as.dist(1 - crude.cos)) # use this line for Cosine
plot(season.hc)

season.clust <- cutree(season.hc, k = 3)
season.clust

season.km <- kmeans(season.tfidf, centers = 3)
season.km$cluster


data.frame(
  Clust.1 = names(sort(apply(season.tfidf[season.clust==1, ], 2, sum), decreasing = TRUE)[1:5]),
  Clust.2 = names(sort(apply(season.tfidf[season.clust==2, ], 2, sum), decreasing = TRUE)[1:5]),
  Clust.3 = names(sort(apply(season.tfidf[season.clust==3, ], 2, sum), decreasing = TRUE)[1:5])
)
```


## Similarities between words
We use the cosine distance measure to determine the similarities between words. 
```{r}
season.feat <- textstat_frequency(season.dfm) %>%
  filter(rank <= 50) 
season.feat$feature

season.euc <- textstat_simil(
  season.dfm[, season.feat$feature],
  method = "cosine",
  margin = "feature")

season.cos.mat <- melt(as.matrix(season.cos)) # Convert the object to matrix then to data frame 

ggplot(data = season.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
    axis.text.y = element_text(size = 5)) +
  xlab("") + 
  ylab("")

```

## Clustering words
```{r}
season.hcw <- hclust(as.dist(1 - season.cos))
plot(season.hcw)
```

## Co-occurences --> Doesn't work
```{r}
season.fcm <- fcm(season.tk, 
                 window = 3, 
                 tri = FALSE)
season.fcm <- (season.fcm + t(season.fcm))/2 ## make the co-occurrence matrix symmetrical

season.fcm.mat <- melt(
  as.matrix(
    season.fcm[season.feat$feature, season.feat$feature]),
  varnames = c("Var1", "Var2")) 
ggplot(data = season.fcm.mat, 
       mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 140,
    limit = c(0, 280),
    name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
        axis.text.y = element_text(size = 5)) +
  xlab("") +
  ylab("")
```

## Cluster dendrogram
```{r}
season.inv_occ <- 
  280 - as.matrix(
    season.fcm[season.feat$feature, season.feat$feature]) ## 280 is the max co-occurrence here
season.hc <- hclust(as.dist(season.inv_occ))
plot(season.hc)
```


## LSA on TF
```{r}
season.tf <- dfm(season.tk)
season.lsa <- textmodel_lsa(x = season.tf, nd = 10) 
head(season.lsa$docs)
```

```{r}
head(season.lsa$features)
```

```{r}
season.freq <- ntoken(season.tk) # row-sum of the DTM. Are you convinced it is the document length?
data.frame(season.freq,
           dim1 = season.lsa$docs[, 1]) %>% 
  ggplot(aes(season.freq, dim1)) + 
  geom_point() + 
  xlab("Number of tokens") + 
  ylab("LSA dim. 1")
```

```{r}
n.terms <- 5
## For Dimension 2
w.order <- sort(season.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
## For Dimension 3
w.order <- sort(season.lsa$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

```

```{r}
w.top2
```

```{r}
w.top3
```

```{r}
biplot(
  y = season.lsa$docs[, 2:3],
  x = season.lsa$features[, 2:3],
  col = c("black", "red"),
  cex = c(0.3, 0.3),
  xlab = "Dim 2",
  ylab = "Dim 3")
```

```{r}
w.subset <- 
  season.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]
biplot(
  y = season.lsa$docs[, 2:3],
  x = w.subset,
  col = c("black","red"),
  cex = c(0.5, 0.5),
  xlab = "Dim 2",
  ylab = "Dim 3")
```


## LSA on TF_IDF ?? Do we want to do that ?
```{r}

```

## LDA using quanteda
```{r}
set.seed(1234) #To create reproducible results
season.lda <- textmodel_lda(x = season.tf, k = 10)
seededlda::terms(season.lda, 5)
```

```{r}
seededlda::topics(season.lda)
```

```{r}
seededlda::topics(season.lda) %>% table()
```

## Term-Topic Analysis
```{r}
phi.long <- melt(
  season.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```


## Topic-Document Analysis
```{r}
set.seed(1234)
theta.long <- melt(
  season.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

```{r}
## 10 longest documents
## ntoken compute the lengths. They are sorted in decreasing order. 
## We take the 12 first
n.doc <- 12
doc.list <- names(
  sort(ntoken(season.tk), decreasing = TRUE)[1:n.doc])
doc.list


theta.long %>% 
  filter(Doc %in% doc.list) %>%  
  ggplot(aes(reorder_within(Topic, Theta, Doc), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Doc, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") +
   theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```


## LDA diagnostics
```{r}
rev(sort(colSums(season.lda$theta)/sum(season.lda$theta)))
```

```{r}
season.codo <- fcm(
  season.tf, 
  context = "document",
  count = "boolean",
  tri = FALSE) # co-document frequencies
term.mat <- seededlda::terms(season.lda, 5)
Coh <- rep(0, 10)
names(Coh) <- paste0("Topic", 1:10)
for (k in 1:10) {
  D.mat <- t(season.codo[term.mat[,k], term.mat[,k]])
  D.vec <- season.tf %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[, k]) %>% 
    data.frame %>%
    select(feature, docfreq)
  for (m in 2:5){
    for (l in 1:(m - 1)) {
      vm <- term.mat[m, k]
      vl <- term.mat[l, k]
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
    }
  }
}
rev(sort(Coh))
```

```{r}
as.matrix(season.codo[term.mat[, 3], term.mat[, 3]])
```

```{r}
as.matrix(season.codo[term.mat[, 5], term.mat[, 5]])
```

```{r}
excl <- rep(0, 10)
names(excl) <- paste0("Topic", 1:10)
for (k in 1:10) {
  for (i in 1:length(term.mat[,k])) {
    term.phi <- filter(phi.long, Term == term.mat[i,k])
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic1")$Phi / sum(term.phi$Phi)
  }
  excl[k] <- excl[k] / length(term.mat[, k])
}
rev(sort(excl))
```
The most exclusive topic is Topic 1, with five top terms are more specific to it.


## LDA with Topic Model
```{r}
season.LDA <- LDA(
  convert(season.tf, to = "topicmodels"), k = 10)
topicmodels::terms(season.LDA, 5)
```

```{r}
topicmodels::topics(season.LDA)
```

```{r}
topicmodels::topics(season.LDA) %>% table()
```

```{r}
topic_diagnostics(
  topic_model = season.LDA, 
  dtm_data = convert(season.tf, to = "topicmodels"))
```

```{r}
beta.long <- tidy(
  season.LDA,
  matrix = "beta") # equivalent to melt (with this package)

beta.long %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ggplot(aes(reorder_within(term, beta, topic), beta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme(
    axis.text.y = element_text(size = 5),
    axis.text.x = element_text(size = 5),
    strip.text = element_text(size = 5))
```

######################################### Supervised Learning ##########################################

### Supervised Learning 

#Features: DTM and LSA
```{r}
# Get the script per character and tokenize this
character.cp <- corpus(character, text_field = "character_scripts")

#Tokenize: (We may need to discuss how we want the data to be cleaned, now I only remove the punctuation, symbols, and stop words.)
character.tok <- tokens(character.cp, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE)

#due to wordstemming it seems that the character.tok file get some nan values and incomplete sentences.
character.tok <- tokens_tolower(character.tok) %>%
  tokens_wordstem() %>% 
  tokens_remove(stopwords("english"))

# The variable to predict is the character name:
y <- factor(docvars(character.tok, "character_name"))

#Compute the DTM matrix and show the dimensions:
character.dfm <- dfm(character.tok)
dim(character.dfm)
```

```{r}
#Using a LSA to reduce dimensions, the nd should be played with to try different results:
character.lsa <- textmodel_lsa(character.dfm, nd=25)
head(character.lsa$docs) #In original file this was $docs, see what works.

```

```{r}
#Training the classifier:
character.df <- data.frame(Class=y, X=character.lsa$docs) #same here, $docs might need to be changed
index.tr <- sample(size=round(0.8*length(y)), x=c(1:length(y)), replace=FALSE)
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]

#train(df.tr) #This is used to estimate the parameters for the given model from a set of data
character.fit <- ranger(Class ~ ., 
                     data = df.tr)
pred.te <- predict(character.fit, df.te)

confusionMatrix(data=pred.te$predictions, reference = df.te$Class) #This gives the confusionmatrix, precision, specifity, and sensitivity
```


# Improving the features; Takes really long!
```{r}

## Same as above chunck, but now it tries different nd/number of dimensions, may take a long time
nd.vec <- c(2,5,25,50,100, 500, 1000)
acc.vec <- numeric(length(nd.vec))
for (j in 1:length(nd.vec)){
  character.lsa <- textmodel_lsa(character.dfm, nd=nd.vec[j])
  character.df <- data.frame(Class=y, X=character.lsa$docs)
  df.tr <- character.df[index.tr,]
  df.te <- character.df[-index.tr,]
  
  character.fit <- ranger(Class ~ ., 
                       data = df.tr)
  pred.te <- predict(character.fit, df.te)
  acc.vec[j] <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)$overall[1]
}
## Growing trees.. Progress: 93%. Estimated remaining time: 2 seconds.
acc.vec
## [1] 0.7942547 0.8625776 0.8819876 0.8827640 0.8804348 0.8812112 0.8796584
plot(acc.vec ~ nd.vec, type='b')

```


```{r}
## compare to tf-idf, results in a little higher accuracy
character.tfidf <- dfm_tfidf(character.dfm)
character.lsa <- textmodel_lsa(character.tfidf, nd=25)

df <- data.frame(Class=y, X=character.lsa$docs)
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]
character.fit <- ranger(Class ~ ., 
                     data = df.tr)
pred.te <- predict(character.fit, df.te)
confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
```

## Word Embedding with glove
```{r}
character.fcm <- fcm(character.tok, 
           context = "window",
           count = "weighted",
           window=5,
           weights = 1/(1:5),
           tri = FALSE)
glove <- GlobalVectors$new(rank = 25, x_max = 1)

word_vectors_main <- glove$fit_transform(character.fcm, n_iter = 100)

```

Takes very long as well, results of accuracy are worse than the previous one based on tf-idf 
> Question for the teachter: I added [complete.cases(df.te),] and [complete.cases(df.tr),], to not include the NAN values and thus making

The script works. How does this influence the results?
```{r}

word_vectors_context <- glove$components
character.glove <- word_vectors_main + t(word_vectors_context)

ndoc <- length(character.tok) # number of documents
centers <- matrix(nr=ndoc, nc=25) # document embedding matrix (1 document per row)
for (i in 1:ndoc){
  words_in_i <- character.glove[character.tok[[i]],, drop=FALSE]
  centers[i,] <- apply(words_in_i,2,mean)
}
row.names(centers) <- names(character.tok)

#any(is.na(words_in_i))

character.df <- data.frame(Class=y, X=centers)
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]
character.fit <- ranger(Class ~ ., 
                     data = df.tr[complete.cases(df.tr),])
pred.te <- predict(character.fit, df.te[complete.cases(df.te),])
confusionMatrix(data=pred.te$predictions, reference = df.te[complete.cases(df.te),]$Class)

```


In this case we add the length of the sentences, by using log (which they did in the example of week 8) it decreases the accuracy.
Without the accuracy is a bit improved.
```{r}
character.tfidf <- dfm_tfidf(character.dfm)
character.lsa <- textmodel_lsa(character.tfidf, nd=25)

character.df <- data.frame(Class=y, X=character.lsa$docs)
character.df <- cbind(character.df,
            length = sapply(character.tok, length))
df.tr <- character.df[index.tr,]
df.te <- character.df[-index.tr,]
character.fit <- ranger(Class ~ ., 
                     data = df.tr[complete.cases(df.tr),], 
                     importance = "impurity")
pred.te <- predict(character.fit, df.te[complete.cases(df.te),])
confusionMatrix(data=pred.te$predictions, reference = df.te[complete.cases(df.te),]$Class)
```


Further improvement based on adding the centroids (and the extra feature from above is used). This results gives the highest accuracy so far.
```{r}

character2.df <- cbind(character.df, Cent=centers) ## add the centroid features to the tf-idf-lsa, retweet, length features.
df.tr <- character2.df[index.tr,]
df.te <- character2.df[-index.tr,]
character.fit <- ranger(Class ~ ., 
                     data = df.tr[complete.cases(df.tr),], 
                     importance = "impurity")
pred.te <- predict(character.fit, df.te[complete.cases(df.te),])
confusionMatrix(data=pred.te$predictions, reference = df.te[complete.cases(df.te),]$Class)

```


# Trying code to put in sentences to predict
```{r}
char_pred <- data.frame(Predicted_sentence = c("Put here the sentence you want to predict"))
char_pred <- do.call("rbind", replicate(5, char_pred, simplify = FALSE))



char_pred.cp <- corpus(char_pred, text_field = "Predicted_sentence")

char_pred.tok <- tokens(char_pred.cp, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE)

char_pred.tok <- tokens_tolower(char_pred.tok) %>% tokens_wordstem() %>% #due to wordstemming it seems that the character.tok file get some nan values and incomplete sentences.
  tokens_remove(stopwords("english"))

char_pred.dfm <- dfm(char_pred.tok)
dim(char_pred.dfm)

char_pred.tfidf <- dfm_tfidf(char_pred.dfm)
char_pred.lsa <- textmodel_lsa(char_pred.tfidf, nd=25)

char_pred.lsa <- textmodel_lsa(char_pred.dfm, nd=25)
head(char_pred.tfidf$docs) #In original file this was $docs, see what works.

z <- c("Howard", "Leonard", "Penny", "Raj", "Sheldon")
haracter.df <- data.frame(Class=z, X=char_pred.lsa$docs)
character.df <- cbind(character.df)

character_prediction <- predict(character.fit, char_pred.dfm)
```


## Random Forest 
```{r}
# randomForest
pred_randomForest <- predict(ames_randomForest, ames_test)
head(pred_randomForest)
##        1        2        3        4        5        6 
## 128266.7 153888.0 264044.2 379186.5 212915.1 210611.4

# ranger
pred_ranger <- predict(character.fit, ames_test)
head(pred_ranger$predictions)
## [1] 128440.6 154160.1 266428.5 389959.6 225927.0 214493.1

```


####################################### Sentiment Analysis by Characters ###############################################

```{r}
char.tb <- as_tibble(data.frame(character_speech))

words_to_remove <- c("scene", "series", "episode", "pilot", "yeah", "uh", "hey",
                    "gonna", "knock", "dr", "apartment", "um", "wanna", "door")

char.tok <- unnest_tokens(
  char.tb,
  output = "word",
  input = "character_scripts",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)
 
char.tok <- char.tok %>%
  anti_join(stop_words, by = c("word" = "word"))
  
char.sent <- 
  inner_join(
    char.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))

head(char.sent, 10) %>% 
  flextable() %>%
  autofit()
```

```{r}
## Long format
char.sent %>% 
  group_by(character_name, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~character_name) + 
  coord_flip()
```

## Sentiments of each characters by using nrc lexicon 

```{r}
char.sent %>% 
  filter(char.sent %in% c("sheldon","leonard","penny","howard","raj")) %>% 
  ggplot(aes(sentiment, fill = character_name))+
  geom_bar(show.legend = FALSE)+
  facet_wrap(character_name~.)+
  theme_dark()+
  theme(
    strip.text = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
  )+
  labs(fill = NULL, x = NULL, y = "Sentiment Frequency", title = "Sentiments of each characters by using nrc lexicon")+
  scale_fill_manual(values = c("#EA181E", "#00B4E8", "#FABE0F","#EA181E", "#00B4E8", "#FABE0F"))
```

## Negative-Positive Ratio in all seasons by using bing lexicon
```{r}
tidy_bing %>% 
  filter(author %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  group_by(season, author) %>% 
  count(sentiment) %>%
  ungroup() %>%
  ggplot(aes(season, n, fill = sentiment)) +
  geom_col(position = "fill") +
  geom_text(aes(label = n), position = position_fill(0.5), color = "white")+
  coord_flip()+
  facet_wrap(author~.)+
  theme_dark()+
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold")
    )+
  scale_fill_manual(values = c("#EA181E", "#00B4E8"))+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10))+
  labs(y = NULL,  x = "Season", fill = NULL, title = "Negative-Positive Ratio in all seasons by using bing lexicon")
```

## Total sentiment score
```{r}
character_speech %>% 
  filter(character_name %in% c("Ross", "Monica", "Rachel", "Joey", "Chandler", "Phoebe")) %>% 
  group_by(season, character_name) %>% 
  summarise(total = sum(value), .groups = 'drop') %>% 
  ungroup() %>% 
  mutate(Neg = if_else(total < 0, TRUE, FALSE)) %>% 
  ggplot()+
  geom_path(aes(season, total, color = characters_names), size = 1.2)+
  theme_minimal()+
  theme(legend.position = "bottom")+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10))+
  scale_color_manual(values = c("#EA181E", "#00B4E8", "#FABE0F", "seagreen2", "orchid", "royalblue"))+
  labs(x = "Season", color = NULL, y = "Total Sentiment Score")+
  annotation_custom(img, ymin = 350, ymax = 400, xmin = 1, xmax = 4)
```

