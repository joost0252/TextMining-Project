---
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```

```{r echo=FALSE}
data_scripts <- read.csv("../data/series_scripts.csv")
season_scripts <- read.csv("../data/season_scripts.csv")
character_speech <- read.csv("../data/character_speech.csv", stringsAsFactors = FALSE)
```

### Sentiment Analysis
In this following part, we want to compute the sentiment of each season. To do so, we decide to first used the 'NRC' dictionary to perform the analysis. Then we want to compare our results to another analysis using another dictionary, the 'afinn' dictionary. 


#### NRC dictionary
The NRC dictionary contains a list of English words and their associations with eight basic emotions and two sentiments (positive or negative). These emotions are anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. Based on this dictionary, one English word can be associated to several emotions. For example, we see on the following table that the term 'abandon' is associated to several emotions (fear, negative, sadness). 

```{r echo=FALSE, include=FALSE}
head(get_sentiments(lexicon = "nrc"))
```


For each token in our data scripts, we join the corresponding sentiment qualifier in “nrc” using the `inner.join()` function from `dplyr`:
Below, you can see the first 10 rows of the dictionary. 

```{r echo=FALSE}
season.tb <- as_tibble(data.frame(season_scripts))

words_to_remove <- c("scene", "series", "episode", "pilot", "yeah", "uh", "hey",
                    "gonna", "knock", "dr", "apartment", "um", "wanna", "door")

season.tok <- unnest_tokens(
  season.tb,
  output = "word",
  input = "agg_script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)
 
season.tok <- season.tok %>%
  anti_join(stop_words, by = c("word" = "word"))

season.tok <- season.tok %>% 
  filter(!word %in% words_to_remove) %>% 
  mutate(word = str_remove_all(word, "'s")) 
  
season.sent <- 
  inner_join(
    season.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))

head(season.sent, 10) %>% 
  flextable() %>%
  autofit()
```

Here, we show the overall sentiment per season. It seems that season 2 and 6 has very few sentiments.
## Problem: graph is the same everywhere
```{r echo=FALSE}
table(season.sent$season, 
      season.sent$sentiment)

## Long format
season.sent %>% 
  group_by(season, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ season ) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):
```{r}
#totals by document
season.sent.doc.total <- 
  season.sent %>% 
  group_by(season) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  season.sent,
  season.sent.doc.total
  ) %>% 
  group_by(season, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ season) + 
  coord_flip()
```


By re-scaling, we see that all seasons follow the same pattern, meaning that they are mainly positive and reflecting the sentiment of trust while very few of disgust sentiments appear.

#### AFINN dictionary
Now, on this part, we will us the AFINN dictionary. This dictionary contains a list of English words manually rated for valence with an integer between -5 (very negative) and +5 (very positive) by Finn Årup Nielsen.


We see on the below table that the word 'abandoned' has a value of -2 which means that it is relatively negative. 

```{r echo=FALSE}
head(get_sentiments("afinn"))
```

We see there on the table below that the seasons 3, 2 and 7 are categorized as relatively negative with season 3 being the most negative. 

```{r echo=FALSE}
season.sent2 <-
  inner_join(
    season.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
season.sent2 %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r echo=FALSE}
season.sent2 %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(season, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```

## Quanteda analysis
```{r echo=FALSE}
season.cp <- corpus(season_scripts$agg_script)
summary(season.cp)

season.tk <- tokens(
  season.cp, 
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE)

season.tk <- tokens_tolower(season.tk) 

season.tk <- tokens_replace(
  season.tk,
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)

season.tk <- season.tk %>% 
  tokens_remove(words_to_remove)
#season.tk

tokens_lookup(
  season.tk,
  dictionary = data_dictionary_LSD2015
  ) 


season.sent1 <- tokens_lookup(
  season.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()

#This part can be removed, it is repetitive
# season.sent1 %>% 
#   pivot_wider(
#     names_from = "term",
#     values_from = "count"
#   ) %>% 
#   mutate(negative = replace_na(negative, 0),
#          Score = round(negative / (negative + positive), 3)) %>% 
#   arrange(Score) %>% 
#   head(20) %>% 
#   flextable() %>% 
#   autofit() 
```

From another dictionary named 'data_dictionary_LSD2015', we see pretty much the same analysis. 

```{r}
ggplot(season.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```

## Valence-Shifters analysis

```{r echo=FALSE, include=FALSE}
hash_sentiment_jockers_rinker
hash_valence_shifters
```

