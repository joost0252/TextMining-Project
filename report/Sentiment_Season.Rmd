---
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```
# Part 3 : Ananlysis 

## Part 3.1 Sentiment Analysis by season
```{r echo=FALSE}
data_scripts <- read.csv("../data/series_scripts.csv")
season_scripts <- read.csv("../data/season_scripts.csv")
character_speech <- read.csv("../data/character_speech.csv", stringsAsFactors = FALSE)
```

### Sentiment Analysis
In this following part, we want to compute the sentiment of each season. To do so, we decide to first used the 'NRC' dictionary to perform the analysis. Then we want to compare our results to another analysis using another dictionary, the 'afinn' dictionary. 


#### NRC dictionary
The NRC dictionary contains a list of English words and their associations with eight basic emotions and two sentiments (positive or negative). These emotions are anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. Based on this dictionary, one English word can be associated to several emotions. For example, we see on the following table that the term 'abandon' is associated to several emotions (fear, negative, sadness). 

```{r echo=FALSE, include=FALSE}
head(get_sentiments(lexicon = "nrc"))
```


For each token in our data scripts, we join the corresponding sentiment qualifier in “nrc” using the `inner.join()` function from `dplyr`:
Below, you can see the first 10 rows of the dictionary. 

```{r echo=FALSE}
season.tb <- as_tibble(data.frame(season_scripts))

words_to_remove <- c("scene", "series", "episode", "pilot", "yeah", "uh", "hey",
                    "gonna", "knock", "dr", "apartment", "um", "wanna", "door")

season.tok <- unnest_tokens(
  season.tb,
  output = "word",
  input = "agg_script",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)
 
season.tok <- season.tok %>%
  anti_join(stop_words, by = c("word" = "word"))

season.tok <- season.tok %>% 
  filter(!word %in% words_to_remove) %>% 
  mutate(word = str_remove_all(word, "'s")) 
  
season.sent <- 
  inner_join(
    season.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))

head(season.sent, 10) %>% 
  flextable() %>%
  autofit()
```

Here, we show the overall sentiment per season. It seems that season 2 and 6 has very few sentiments.
## Problem: graph is the same everywhere
```{r echo=FALSE}
table(season.sent$season, 
      season.sent$sentiment)

## Long format
season.sent %>% 
  group_by(season, sentiment) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ season ) + 
  coord_flip()
```

To compare the documents, we rescale them by their length (i.e. the frequencies of sentiments are computed, by document):
```{r}
#totals by document
season.sent.doc.total <- 
  season.sent %>% 
  group_by(season) %>% 
  summarize(Total = n()) %>% 
  ungroup()

#add totals and compute relative frequency
left_join(
  season.sent,
  season.sent.doc.total
  ) %>% 
  group_by(season, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ season) + 
  coord_flip()
```


By re-scaling, we see that all seasons follow the same pattern, meaning that they are mainly positive and reflecting the sentiment of trust while very few of disgust sentiments appear.

#### AFINN dictionary
Now, on this part, we will us the AFINN dictionary. This dictionary contains a list of English words manually rated for valence with an integer between -5 (very negative) and +5 (very positive) by Finn Årup Nielsen.


We see on the below table that the word 'abandoned' has a value of -2 which means that it is relatively negative. 

```{r echo=FALSE}
head(get_sentiments("afinn"))
```

We see there on the table below that the seasons 3, 2 and 7 are categorized as relatively negative with season 3 being the most negative. 

```{r echo=FALSE}
season.sent2 <-
  inner_join(
    season.tok, 
    get_sentiments("afinn"),
    by = c("word" = "word"))

## Summarize per document (value average) + barplot
season.sent2 %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```

```{r echo=FALSE}
season.sent2 %>% 
  group_by(season) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(season, Score), y = Score)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  ylab("Mean Sentiment Score") +
  xlab("")
```
From this analysis we can observe that the second half of the show, except for season 7, has a more positive score. We can suppose that the characters tend to become more positive and maybe less sarcastic as the series goes on  

## Quanteda analysis
```{r echo=FALSE}
season.cp <- corpus(season_scripts$agg_script)
summary(season.cp)

season.tk <- tokens(
  season.cp, 
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE)

season.tk <- tokens_tolower(season.tk) 

season.tk <- tokens_replace(
  season.tk,
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)

season.tk <- season.tk %>% 
  tokens_remove(words_to_remove)
#season.tk

tokens_lookup(
  season.tk,
  dictionary = data_dictionary_LSD2015
  ) 


season.sent1 <- tokens_lookup(
  season.tk, 
  dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>% 
  tidy()

#This part can be removed, it is repetitive
# season.sent1 %>% 
#   pivot_wider(
#     names_from = "term",
#     values_from = "count"
#   ) %>% 
#   mutate(negative = replace_na(negative, 0),
#          Score = round(negative / (negative + positive), 3)) %>% 
#   arrange(Score) %>% 
#   head(20) %>% 
#   flextable() %>% 
#   autofit() 
```

From another dictionary named 'data_dictionary_LSD2015', we see pretty much the same analysis. 

```{r}
ggplot(season.sent1,
       aes(x = document,
           y = count,
           fill = term)) + 
  geom_bar(stat="identity") + 
  coord_flip()
```


## Valence-Shifters analysis

```{r echo=FALSE, include=FALSE}
hash_sentiment_jockers_rinker
hash_valence_shifters
```

Valence shifters are words that alter or intensify the meaning of the polarized words and include negators and amplifiers. 

x. Valence shifter
y. Number key value corresponding to:

Valence Shifter	Value
Negator	1
Amplifier (intensifier)	2
De-amplifier (downtoner)	3
Adversative Conjunction	4

```{r echo=FALSE, include=FALSE}

sentiment(season_scripts$agg_script)
sentiment_by(season_scripts$agg_script, by = NULL)
profanity(season_scripts$agg_script)


season_with_pol <- season_scripts %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
                                 ifelse(sentiment > 0.2, "Positive","Neutral")))
           
season_with_pol %>% filter(polarity_level == "Negative") %>% View()

# Look in an html file every script with sentiments appearing  
#season_scripts$agg_script %>% 
  #get_sentences() %>% 
  #sentiment_by() %>% #View()
  #highlight()

```

```{r echo=FALSE} 
season_scripts$agg_script %>% 
  get_sentences() %>% 
  sentiment_by(by = NULL) %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))
```
We can see with this density graph, the average sentiment on the wole season, with the valence shifters. It tends on average to be superior to 0, the mean around 0.8.

# Valence shifter approach on each season 
```{r echo=FALSE, include=FALSE}
season.script.text <- get_sentences(season_scripts)
season.script.senti <- sentiment(season.script.text) 
```

```{r echo=FALSE}
season.script.senti %>% group_by(element_id) %>% 
  ggplot(aes(x = sentence_id, y = sentiment)) + 
  geom_line() +  
  facet_wrap(~ element_id, scale="free_x")

```
The analysis is possible by going through each sentence. Looking at season 9 for instance, the end of the show looks to be very positive with an increasing trend. The pattern shows that sometimes we have high peaks like in season 4. And sometimes very low peaks but it remains well distributed. Indeed if we take a look at season 10, we have 4 drops in the negative and it looks to appear every 1000 sentences. 

```{r echo=FALSE, include=FALSE}
sentiment_by(season.script.senti)
```

```{r echo=FALSE}
sentiment_by(season.script.senti) %>% 
  mutate(Doc = docnames(season.cp)) %>%
  ggplot(aes(x = reorder(Doc, ave_sentiment),
             y = ave_sentiment)) + 
  ylab("Avg sentiment") +
  geom_bar(stat = "identity") + 
  coord_flip() + 
  xlab("") 
```
With this barplot, we can once again identify the scaled average sentiment and see that the most positive season is 10 and the least positive season is 2. The main difference with the valence shifter shows that season 10 is way more positive with this analysis compared to the previous one. It shows how important we must consider this aspect. 

